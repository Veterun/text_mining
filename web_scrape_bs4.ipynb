{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Web Scraping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "14D010 - Text Mining for Social Sciences\n",
      "Masters of Data Science, Barcelona Graduate School of Economics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Software Prequistes - Python 2.7.6\n",
      "\n",
      "Packages: \n",
      "1. BeautifulSoup4\n",
      "2. urllib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Design consideratons\n",
      "\n",
      "The principal theme of this notebook is to demonstrate the use of beautifulsoup4 library to \"scrape\" content from html websites. Most information in a html source is of little use to us and is used to render and format the webpage itself. Therefore it would be prudent to familiarise yourself with HTML tags as and when needed. A good starting point is [w3schools' html tutorial](http://www.w3schools.com/html/)\n",
      "\n",
      "Before we get into the nitty gritties, we must consider some design aspects of creating a \"spider\" that will \"crawl\" the target webpage and get us the desired content. \n",
      "\n",
      "1. __Identify tags that contain useful information__\n",
      "By exploiting the consistencies in HTML design of a website we can identify tags that contain useful information.\n",
      " \n",
      "2. __Add randomised waiting periods between every access to website__\n",
      "Most websites are not a big fan of bots scraping data. Reasons range from protecting data to avoiding scraping attempts that flood their servers with servers, effecively mounting a DOS(denial of service) attack. So be careful! Be very very careful. Measures against scraping could be legal notices in the worst case scenario. Normally, the ip address of the origin of requests is blocked. (This is not good if you're working from UPF!)  \n",
      "\n",
      "3. __Make use of logs to monitor progress__\n",
      "Clearly, scraping is needed to locate and download tonnes of data. Which means that scripts could be running for long sessions. Last year it was typical for our scripts to run for hours if not days. Which is why it is of utmost importance to keep track of all success and failures to selectively rerun scripts.\n",
      "\n",
      "4. __Regularly write collected data to file__\n",
      "This avoids overloading memory and protects against unexpected termination of scripts. You can choose several ways to save your data. As a serialsed python object, csv, text, etc.\n",
      "\n",
      "5. __ALWAYS, and I mean ALWAYS respect robot.txt file defined by websites__\n",
      "Robot.txt files, maintained by different websites, lay down the ground rules for a properly behaved web spider/crawler. Make sure you have a close look at it before designing your script. Follow [this link](https://en.wikipedia.org/robots.txt) for Wikipedia's robot.txt file.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Example:  Wikipedia\n",
      "\n",
      "__Target__: To retrieve the summary of the featured article in Wikipedia along with all the urls in it. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import urllib\n",
      "\n",
      "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
      "html = urllib.urlopen(url).read()\n",
      "soup = BeautifulSoup(html)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variable html is a string whose contents include the entire html contents of the page. We need a way to parse this text and exploit the hml code's structural consistencies to get to the information we need. As the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) explains, Beautiful Soup is a Python library for pulling data out of HTML and XML files. I encourage you to look at it and familiarise yourself with various ways to search for patterns. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create a beautifulSoup Object that will be used to retieve information from the target\n",
      "soup = BeautifulSoup(html)\n",
      "print soup.prettify()[0:1000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<!DOCTYPE html>\n",
        "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
        " <head>\n",
        "  <meta charset=\"utf-8\"/>\n",
        "  <title>\n",
        "   Wikipedia, the free encyclopedia\n",
        "  </title>\n",
        "  <script>\n",
        "   document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
        "  </script>\n",
        "  <script>\n",
        "   (window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Main_Page\",\"wgTitle\":\"Main Page\",\"wgCurRevisionId\":696846920,\"wgRevisionId\":696846920,\"wgArticleId\":15580374,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"D\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As evident the text is structured with HTML tags organised in a systematic way. This structure is referred to as a HTML DOM(Document Object Model). For a brief introduction refer to <a href = \"http://www.w3schools.com/js/js_htmldom.asp\"> W3's introduction to HTML DOM. </a>\n",
      "\n",
      "An example of DOM is as follows:\n",
      "<img src=\"http://www.w3schools.com/js/pic_htmltree.gif\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are interested grabbing the summary of article of the day, from the [Wikipedia main page](https://en.wikipedia.org/wiki/Main_Page) along with all the links in it. So we'll use BeautifulSoup's functions to parse HTML tags that contain this information or lead us to it in the form of embedded urls.\n",
      "\n",
      "On inspection of the HTML source we identify that the article summary is enclosed in the tag _table_ with the attribute _id = mp-upper_ .\n",
      "The idea is as follows, \n",
      "\n",
      "1. Using the function in _find_all_ in BeautifulSoup we search for all tags of the type _table_\n",
      "2. Search the returned list for all the tag that has an attribute _id = mp-upper_.\n",
      "3. Now that you have the table notice that the information needed is in the tag called _p_. Search for all tags _p_\n",
      "4. Identify the correct paragraph and extract urls.\n",
      "5. Extract text\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1. search for all tags called table\n",
      "tables_list = soup.find_all(name = \"table\")\n",
      "\n",
      "\n",
      "#2. search for the table interest in the returned list by matching on the key 'id'\n",
      "for table in tables_list:\n",
      "    try:\n",
      "        if table['id'] == 'mp-upper':\n",
      "            article_table = table\n",
      "    except:\n",
      "        None\n",
      "\n",
      "#3. Search for the tag p\n",
      "print 'object type of article_table is ' + str(type(article_table)) + '\\n'\n",
      "paragraph = article_table.findAll(name = 'p')[0]\n",
      "\n",
      "#4. Extract urls\n",
      "urls = [tag['href']for tag in paragraph.findAll('a', href = True)]\n",
      "for url in urls:\n",
      "    print url\n",
      "print '\\n'\n",
      "\n",
      "#5 Now we construct the text of the article summary by looping through al the children and concatinating the text \n",
      "text = ''\n",
      "for ch in paragraph.children:\n",
      "    text = text + ch.string\n",
      "print(text)\n",
      "print '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "object type of article_table is <class 'bs4.element.Tag'>\n",
        "\n",
        "/wiki/Masked_shrike\n",
        "/wiki/Shrike\n",
        "/wiki/Mediterranean\n",
        "/wiki/Genus\n",
        "/wiki/Supercilium\n",
        "/wiki/Flight_feather#Primaries\n",
        "/wiki/Old_World_warbler\n",
        "/wiki/International_Union_for_Conservation_of_Nature\n",
        "/wiki/Species_of_least_concern\n",
        "/wiki/Masked_shrike\n",
        "\n",
        "\n",
        "The masked shrike (Lanius nubicus) is a bird in the shrike family, Laniidae. It breeds in southeastern Europe and at the eastern end of the Mediterranean, with a separate population in eastern Iraq and western Iran, and winters mainly in northeast Africa. It is the smallest member of its genus, long-tailed and with a hooked bill. The male has mainly black upperparts, with white on its crown, forehead and supercilium and large white patches on the shoulders and wings. The throat, neck sides and underparts are white, with orange flanks and breast. The female is a duller version of the male, with brownish black upperparts and a grey or buff tone to the shoulders and underparts. The species' calls are short and grating, but the song has melodic warbler-like components. The masked shrike eats mainly large insects and occasionally small vertebrates; it sometimes impales its prey on thorns or barbed wire. Decreasing in parts of the European range, but not rapidly enough to raise serious conservation concerns, it is classified by the International Union for Conservation of Nature as a species of least concern. (Full\u00a0article...)\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A Toy Web Crawler\n",
      "Below I present a _baby_ web crawler, if you will, to illustrate the different design principles that we have spoken about so far.\n",
      "\n",
      "###Background\n",
      "I created this web crawler to pull out data regarding contracts awarded by the Spanish government in the last 15 years from 200 to 2016. This information is put on the website http://www.boe.es/. This means searching for and identifying about half a million urls each corresponding to a project. Each project webpage had a link to the xml version of the data needed. This slightly increases the complexity of the crawler but is helpful in removing html formatting related noise from the the data. As an example consider this project [webpage](http://www.boe.es//buscar/doc.php?id=BOE-B-2000-1005). Inspecting its [source](view-source:http://www.boe.es//buscar/doc.php?id=BOE-B-2000-1005), you'd see a lot of html tags which are used for formatting the page. Not needed. I bypassed the difficulty of identifying and parsing the correct tag by using the xml links.   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################################################################################\n",
      "#import modules\n",
      "\n",
      "# for establishing a connection between python and the targeted website\n",
      "import urllib2\n",
      "\n",
      "# for retreiving information from websites \n",
      "from bs4 import BeautifulSoup as bs \n",
      "\n",
      "# convert xml as string to an ordered dictionary \n",
      "import xmltodict\n",
      "\n",
      "# to read arguments from command line\n",
      "import os \n",
      "\n",
      "# to use numpy arrays which are more efficient than regular data structures\n",
      "import numpy as np\n",
      "\n",
      "# to read the folder structure of the target\n",
      "import sys\n",
      "\n",
      "# to add random delays in time\n",
      "import time, random\n",
      "\n",
      "\n",
      "#import xml.etree.ElementTree as ET\n",
      "#import json\n",
      "##################################################################################\n",
      "\n",
      "def get_xml_content(url):\n",
      "    #Description: gets xml content from a url supplied\n",
      "    #read supplied url and retrieve html\n",
      "    #input: url - url to the xml version of data\n",
      "    #output: An ordered dict of the xml data\n",
      "    \n",
      "    response = urllib2.urlopen('http://www.boe.es/buscar/doc.php?id=BOE-B-2015-39114')\n",
      "    project_html = response.read()\n",
      "    \n",
      "    #convert html to soup object\n",
      "    soup = bs(project_html, 'html.parser')\n",
      "    \n",
      "    #check if html has xml file\n",
      "    xml_tag = soup.findAll(\"li\", class_ = \"puntoXML\")\n",
      "    \n",
      "    #if so then retrieve the xml url\n",
      "    if len(xml_tag) != 0:\n",
      "        xml_url = xml_tag[0].a['href']\n",
      "        \n",
      "        #convert xml string to ordered dict\n",
      "        xml_response = urllib2.urlopen('https://www.boe.es' + xml_url)\n",
      "        xml_str = xml_response.read()\n",
      "        xml_dict = xmltodict.parse(xml_str)\n",
      "        \n",
      "    return xml_dict\n",
      "\n",
      "\n",
      "def scrape_contracts(xml_urls, file_prefix):\n",
      "    #Uses a list of the xml url provided to scrape data from them and store to disk periodically\n",
      "    #Writes various logs to monitor progress\n",
      "    #input: xml_urls - a list of xml urls\n",
      "    #output - None\n",
      "    project_info = []\n",
      "    \n",
      "    for idx, url in enumerate(xml_urls):\n",
      "        #add a randomly generated sleep time before successive requests\n",
      "        time.sleep(random.randrange(1, 3, 1))\n",
      "        try:\n",
      "            #try to retrieve information from url\n",
      "            project_info.append(get_xml_content(url))\n",
      "            \n",
      "            #add completed url to the log of completed urls\n",
      "            with open(\"./data/projects/completed_urls.txt\", \"a\") as complete_file:\n",
      "                complete_file.write(url + '\\n')\n",
      "                complete_file.close()\n",
      "        except:\n",
      "            #add rejected urls to the log of rejected urls\n",
      "            with open(\"./data/projects/rejected_urls.txt\", \"a\") as rejected_file:\n",
      "                rejected_file.write(url + '\\n')\n",
      "                rejected_file.close()\n",
      "        \n",
      "        if idx % 10000 == 0 and idx != 0:\n",
      "            #periodically write the data to file and reinitialise list for memory management\n",
      "            file_name = './data/projects/'+ file_prefix + str(idx) + '.gz'\n",
      "            np.savetxt(file_name, project_info, delimiter=',', fmt='%s')\n",
      "            project_info = []\n",
      "            \n",
      "            #add the index of last file to be written to disk\n",
      "            with open(\"./data/projects/saved_data_index.txt\", \"a\") as saved_file:\n",
      "                saved_file.write(file_prefix + str(idx) + '\\n')\n",
      "                saved_file.close()\n",
      "    \n",
      "    #Save remaining data to file    \n",
      "    file_name = './data/projects/'+ file_prefix + str(idx) + '.gz'\n",
      "    np.savetxt(file_name, project_info, delimiter=',', fmt='%s')\n",
      "    \n",
      "    \n",
      "def main(argv):\n",
      "    \n",
      "    end_index = int(argv[1])\n",
      "    start_index = int(argv[0])\n",
      "    file_prefix = argv[2]\n",
      "   \n",
      "    xml_urls = np.loadtxt('./data/complete_xml_urls.gz', dtype=str)[start_index:end_index]\n",
      "    scrape_contracts(xml_urls, file_prefix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 104
    }
   ],
   "metadata": {}
  }
 ]
}