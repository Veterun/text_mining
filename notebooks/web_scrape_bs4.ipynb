{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Web Scraping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "14D010 - Text Mining for Social Sciences\n",
      "Masters of Data Science, Barcelona Graduate School of Economics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Software Prequistes - Python 2.7.6\n",
      "\n",
      "Packages: \n",
      "1. BeautifulSoup4\n",
      "2. urllib"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Design consideratons\n",
      "\n",
      "The principal theme of this notebook is to demonstrate the use of beautifulsoup4 library to \"scrape\" content from html websites. Most information in a html source is of little use to us and is used to render and format the webpage itself. Therefore it would be prudent to familiarise yourself with HTML tags as and when needed. A good starting point is [w3schools' html tutorial](http://www.w3schools.com/html/)\n",
      "\n",
      "Before we get into the nitty gritties, we must consider some design aspects of creating a \"spider\" that will \"crawl\" the target webpage and get us the desired content. \n",
      "\n",
      "1. __Identify tags that contain useful information__\n",
      "By exploiting the consistencies in HTML design of a website we can identify tags that contain useful information.\n",
      " \n",
      "2. __Add randomised waiting periods between every access to website__\n",
      "Most websites are not a big fan of bots scraping data. Reasons range from protecting data to avoiding scraping attempts that flood their servers with servers, effecively mounting a DOS(denial of service) attack. So be careful! Be very very careful. Measures against scraping could be legal notices in the worst case scenario. Normally, the ip address of the origin of requests is blocked. (This is not good if you're working from UPF!)  \n",
      "\n",
      "3. __Make use of logs to monitor progress__\n",
      "Clearly, scraping is needed to locate and download tonnes of data. Which means that scripts could be running for long sessions. Last year it was typical for our scripts to run for hours if not days. Which is why it is of utmost importance to keep track of all success and failures to selectively rerun scripts.\n",
      "\n",
      "4. __Regularly write collected data to file__\n",
      "This avoids overloading memory and protects against unexpected termination of scripts. You can choose several ways to save your data. As a serialsed python object, csv, text, etc.\n",
      "\n",
      "5. __ALWAYS, and I mean ALWAYS respect robot.txt file defined by websites__\n",
      "Robot.txt files, maintained by different websites, lay down the ground rules for a properly behaved web spider/crawler. Make sure you have a close look at it before designing your script. Follow [this link](https://en.wikipedia.org/robots.txt) for Wikipedia's robot.txt file.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Example:  Wikipedia\n",
      "\n",
      "__Target__: To retrieve the summary of the featured article in Wikipedia along with all the urls in it. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import urllib\n",
      "\n",
      "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
      "html = urllib.urlopen(url).read()\n",
      "soup = BeautifulSoup(html)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variable html is a string whose contents include the entire html contents of the page. We need a way to parse this text and exploit the hml code's structural consistencies to get to the information we need. As the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) explains, Beautiful Soup is a Python library for pulling data out of HTML and XML files. I encourage you to look at it and familiarise yourself with various ways to search for patterns. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create a beautifulSoup Object that will be used to retieve information from the target\n",
      "soup = BeautifulSoup(html)\n",
      "print soup.prettify()[0:1000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<!DOCTYPE html>\n",
        "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
        " <head>\n",
        "  <meta charset=\"utf-8\"/>\n",
        "  <title>\n",
        "   Wikipedia, the free encyclopedia\n",
        "  </title>\n",
        "  <script>\n",
        "   document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
        "  </script>\n",
        "  <script>\n",
        "   (window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Main_Page\",\"wgTitle\":\"Main Page\",\"wgCurRevisionId\":696846920,\"wgRevisionId\":696846920,\"wgArticleId\":15580374,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"D\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As evident the text is structured with HTML tags organised in a systematic way. This structure is referred to as a HTML DOM(Document Object Model). For a brief introduction refer to <a href = \"http://www.w3schools.com/js/js_htmldom.asp\"> W3's introduction to HTML DOM. </a>\n",
      "\n",
      "An example of DOM is as follows:\n",
      "<img src=\"http://www.w3schools.com/js/pic_htmltree.gif\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are interested grabbing the summary of article of the day, from the [Wikipedia main page](https://en.wikipedia.org/wiki/Main_Page) along with all the links in it. So we'll use BeautifulSoup's functions to parse HTML tags that contain this information or lead us to it in the form of embedded urls.\n",
      "\n",
      "On inspection of the HTML source we identify that the article summary is enclosed in the tag _table_ with the attribute _id = mp-upper_ .\n",
      "The idea is as follows, \n",
      "\n",
      "1. Using the function in _find_all_ in BeautifulSoup we search for all tags of the type _table_\n",
      "2. Search the returned list for all the tag that has an attribute _id = mp-upper_.\n",
      "3. Now that you have the table notice that the information needed is in the tag called _p_. Search for all tags _p_\n",
      "4. Identify the correct paragraph and extract urls.\n",
      "5. Extract text\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1. search for all tags called table\n",
      "tables_list = soup.find_all(name = \"table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#2. search for the table interest in the returned list by matching on the key 'id'\n",
      "for table in tables_list:\n",
      "    try:\n",
      "        if table['id'] == 'mp-upper':\n",
      "            article_table = table\n",
      "    except:\n",
      "        None\n",
      "\n",
      "print 'object type of article_table is ' + str(type(article_table)) + '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#3. Search for the tag p\n",
      "paragraph = article_table.findAll(name = 'p')[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "object type of article_table is <class 'bs4.element.Tag'>\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#4. Extract urls\n",
      "urls = [tag['href']for tag in paragraph.findAll('a', href = True)]\n",
      "for url in urls:\n",
      "    print url\n",
      "print '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/wiki/Aliso_Creek_(Orange_County)\n",
        "/wiki/Urban_stream\n",
        "/wiki/Orange_County,_California\n",
        "/wiki/U.S._state\n",
        "/wiki/California\n",
        "/wiki/Santa_Ana_Mountains\n",
        "/wiki/Pacific_Ocean\n",
        "/wiki/Los_Angeles_Basin\n",
        "/wiki/Sedimentary_rock\n",
        "/wiki/Eocene\n",
        "/wiki/Pliocene\n",
        "/wiki/Ice_Age\n",
        "/wiki/Juane%C3%B1o\n",
        "/wiki/Gabrieleno\n",
        "/wiki/Municipal_water\n",
        "/wiki/Storm_drain\n",
        "/wiki/Biodiversity\n",
        "/wiki/Recreation\n",
        "/wiki/Aliso_Creek_(Orange_County)\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#5 Now we construct the text of the article summary by looping through al the children and concatinating the text \n",
      "text = ''\n",
      "for ch in paragraph.children:\n",
      "    text = text + ch.string\n",
      "print(text)\n",
      "print '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Aliso Creek is a 19-mile (31\u00a0km) urban stream that runs through Orange County in the U.S. state of California from the Santa Ana Mountains to the Pacific Ocean, collecting seven main tributaries. It flows generally south-southwest through a narrow coastal watershed at the southern extreme of the arid Los Angeles Basin in a fairly straight course. Owing to the submersion of Southern California in the Pacific Ocean until 10\u00a0million years ago, the creek flows over marine sedimentary rock that dates from the late Eocene to the Pliocene. The watershed's broad sediment-filled valleys and deeply eroded side canyons were shaped by climate change during the last Ice Age. Historically, the creek served as the boundary between the Juane\u00f1o (Acjachemem) and Gabrieleno (Tongva) Indians. Although attempts to use the creek and its watershed as a municipal water source date to the early 20th\u00a0century, the water it provided was erratic and of poor quality. The creek has become little more than an open wastewater drain, but the watershed supports some biodiversity, and remains a popular recreational area. (Full\u00a0article...)\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A Toy Web Crawler\n",
      "Below I present a _baby_ web crawler, if you will, to illustrate the different design principles that we have spoken about so far.\n",
      "\n",
      "###Background\n",
      "I created this web crawler to pull out data regarding contracts awarded by the Spanish government in the last 15 years from 200 to 2016. This information is put on the website http://www.boe.es/. This means searching for and identifying about half a million urls each corresponding to a project. Each project webpage had a link to the xml version of the data needed. This slightly increases the complexity of the crawler but is helpful in removing html formatting related noise from the the data. As an example consider this project [webpage](http://www.boe.es//buscar/doc.php?id=BOE-B-2000-1005). Inspecting its [source](view-source:http://www.boe.es//buscar/doc.php?id=BOE-B-2000-1005), you'd see a lot of html tags which are used for formatting the page. Not needed. I bypassed the difficulty of identifying and parsing the correct tag by using the xml links.   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################################################################################\n",
      "#import modules\n",
      "\n",
      "# for establishing a connection between python and the targeted website\n",
      "import urllib2\n",
      "\n",
      "# for retreiving information from websites \n",
      "from bs4 import BeautifulSoup as bs \n",
      "\n",
      "#convert xml as string to an ordered dictionary \n",
      "import xmltodict\n",
      "\n",
      "# to read arguments from command line\n",
      "import os \n",
      "\n",
      "#To use numpy arrays which are more efficient than regular data structures\n",
      "import numpy as np\n",
      "\n",
      "#To read the folder structure of the target\n",
      "import sys\n",
      "\n",
      "\n",
      "#import xml.etree.ElementTree as ET\n",
      "#import json\n",
      "##################################################################################\n",
      "\n",
      "def get_xml_content(url):\n",
      "    #Description: gets xml content from a url supplied\n",
      "    #read supplied url and retrieve html\n",
      "    #input: url - url to the xml version of data\n",
      "    #output: An ordered dict of the xml data\n",
      "    \n",
      "    response = urllib2.urlopen(url).read()\n",
      "    xml_dict = xmltodict.parse(response)\n",
      "    return dict(xml_dict)\n",
      "\n",
      "\n",
      "def scrape_contracts(xml_urls, file_prefix):\n",
      "    #Uses a list of the xml url provided to scrape data from them and store to disk periodically\n",
      "    #Writes various logs to monitor progress\n",
      "    #input: xml_urls - a list of xml urls\n",
      "    #output - None\n",
      "    project_info = []\n",
      "    \n",
      "    for idx, url in enumerate(xml_urls):\n",
      "        \n",
      "        try:\n",
      "            #try to retrieve information from url\n",
      "            project_info.append(get_xml_content(url))\n",
      "            \n",
      "            #add completed url to the log of completed urls\n",
      "            with open(\"./data/projects/completed_urls.txt\", \"a\") as complete_file:\n",
      "                complete_file.write(url + '\\n')\n",
      "                complete_file.close()\n",
      "        except:\n",
      "            #add rejected urls to the log of rejected urls\n",
      "            with open(\"./data/projects/rejected_urls.txt\", \"a\") as rejected_file:\n",
      "                rejected_file.write(url + '\\n')\n",
      "                rejected_file.close()\n",
      "        \n",
      "        if idx % 10000 == 0 and idx != 0:\n",
      "            #periodically write the data to file and reinitialise list for memory management\n",
      "            file_name = './data/projects/'+ file_prefix + str(idx) + '.gz'\n",
      "            np.savetxt(file_name, project_info, delimiter=',', fmt='%s')\n",
      "            project_info = []\n",
      "            \n",
      "            #add the index of last file to be written to disk\n",
      "            with open(\"./data/projects/saved_data_index.txt\", \"a\") as saved_file:\n",
      "                saved_file.write(file_prefix + str(idx) + '\\n')\n",
      "                saved_file.close()\n",
      "    \n",
      "    #Save remaining data to file    \n",
      "    file_name = './data/projects/'+ file_prefix + str(idx) + '.gz'\n",
      "    np.savetxt(file_name, project_info, delimiter=',', fmt='%s')\n",
      "    with open(\"./data/projects/saved_data_index.txt\", \"a\") as saved_file:\n",
      "                saved_file.write(file_prefix + str(idx) + '\\n')\n",
      "                saved_file.close()\n",
      "    \n",
      "    \n",
      "def main(argv):\n",
      "    \n",
      "    start_index = int(argv[0])\n",
      "    end_index = int(argv[1])\n",
      "    file_prefix = argv[2]\n",
      "   \n",
      "    xml_urls = np.loadtxt('./data/complete_xml_urls.gz', dtype=str)[start_index:end_index]\n",
      "    scrape_contracts(xml_urls, file_prefix)\n",
      "    \n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    main(sys.argv[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    }
   ],
   "metadata": {}
  }
 ]
}